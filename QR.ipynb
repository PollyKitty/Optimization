{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2b1210",
   "metadata": {},
   "source": [
    "## Линейные системы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ad16e",
   "metadata": {},
   "source": [
    "В лекции про $LU$ разложение мы обуславливались, что в системе $Ax = b$ работаем с квадратными матрицами $A \\in R^{n\\times n}$ и также говорили, что такие системы называются определенными. Но нам также надо считать решение и для других систем, в этом блоке будут разобраны **переопределенные системы**, когда $A \\in R^{m\\times n}, m > n$. Пример такой системы $A \\in R^{3\\times 2}$ (3 наблюдения, 2 атрибута):\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "7 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Мы разумеется могли бы сделать $LU$ разложение для данной матрицы:\n",
    "\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}, b = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "7 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& PA = LU \\\\\n",
    "&\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "4 & -8 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\frac{4}{3} & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "&\\tilde b = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "7 \n",
    "\\end{bmatrix} \\\\\n",
    "& Ly = \\tilde b, \\quad \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\frac{4}{3} & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "7 \n",
    "\\end{bmatrix}, \\quad \\tilde y = (-1, 2, \\frac{25}{3}) \\\\\n",
    "& Ux = \\tilde y, \\quad \\begin{bmatrix}\n",
    "3 & -6 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "\\frac{25}{3}\n",
    "\\end{bmatrix}, \\quad \\tilde x = (\\frac{11}{3}, 2)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Но увы, данное решение нам не подходить, так как в данном случае оно удовлетворяет только 2 наблюдениям из 3, а последнее игнорируется, это можно было заметить в самом начале разложение, где матрица $U$ имела нулевую строку, в общем, любое уравнение вида $A \\in R^{m\\times n}, m > n$ будет иметь $(m-n)$ нулевых строк, а нулевые строчки никак не адаптируют полученное решение из ненулевых строк. То есть решение системы $Ax =b$, полученное с помощью $LU$ алгоритма инвариантно к $(m-n)$ наблюдением и решает задачу только для $n$ ведущих миноров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01c833",
   "metadata": {},
   "source": [
    "### Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b602b",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть матрица $A \\in R^{m\\times n}, m > n$, как мы только что показали, в общем случае мы не можем решить такую систему, и единственное, что мы можем сделать - это минимизировать ошибку нашего решения. Запишем задачу:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{argmin}_{x} ||Ax - b||^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Поиск такого вектора $x$, удовлетворяющего задаче минимизации ошибки выше и называется **методом наименьших квадратов**. Решение такой задачи методом градиентного спуска - тема будующих лекций, но на данном этапе мы рассмотрим следующий способ решения.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&F(x) = ||Ax - b||^2 = (Ax - b)^{\\top}(Ax - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b \\\\\n",
    "&\\nabla F(x) = 2(A^{\\top}Ax - A^{\\top}b) = 0 \\\\\n",
    "&A^{\\top}Ax = A^{\\top}b, \\quad A^{\\top}(Ax - b) = 0\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Последнее уравнение называется **нормальным уравнением**, потому что данное уравнение задает соотношение ортогогнальности между вектором ошибок $r = Ax - b$ и любым вектором $Ay$ из пространства $A$. Нам важно это соотношение тем, что остаток (ошибка) полученная методом наименьштх квадратов не зависит от направлений пространства матрицы $A$, то есть наша ошибка не смещена по какому-либо направлению и не может быть воспроизведена любой линейной комбинацией пространства $A$.\n",
    "\n",
    "#### Решение через ортогональные преобразования\n",
    "\n",
    "Напомню, что класс ортогональных матриц, если вы поворачиваете или отображаете вектор, не меняет длину вектора (его норму), например $||Qy||^2 = (Qy)^{\\top}(Qy) = y^{\\top}Q^{\\top}Qy = ||y||^2$\n",
    "\n",
    "Следовательно, если мы сделаем следующее преобразование, то логика поиска $x$ не изменится: $\\text{argmin}_{x} ||Q^{\\top}(Ax - b)||^2 = \\text{argmin}_x||Q^{\\top}Ax - Q^{\\top}b||^2$, и наш план состоит в том, чтобы применить к A последовательность ортогональных преобразований $Q$, которые приведут еe к верхнетреугольной форме. Это создаст эквивалентную, легко решаемую проблему. Для нашего примера выше:\n",
    "\n",
    "$\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2 = \\text{argmin}_{x\\in\\mathbb{R}^2}\n",
    "\\norm{\n",
    "\\begin{bmatrix}\n",
    "r_{11} & r_{12} \\\\\n",
    "0 & r_{22} \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "\\tilde b_1 \\\\\n",
    "\\tilde b_2 \\\\\n",
    "\\tilde b_3\n",
    "\\end{bmatrix}\n",
    "}^2\n",
    "$\n",
    "\n",
    "Где $\\tilde b = Q^{\\top}b$, заметим, что неважно какие $x_1, x_2$ мы подберем, сумма квадратов ошибок будет как минимум $\\tilde b_3^2$, поэтому мы можем вычеркнуть последнюю строчку и решать следующую систему как обычно:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "r_{11} & r_{12} \\\\\n",
    "0 & r_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde b_1 \\\\\n",
    "\\tilde b_2 \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Таким образром, нам надо свести матрицу $A$ верхнетреугольной форме, но задача легче от этого не становится. Нам все еще надо решить следующее - $\\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2$, то есть подобрать какую-то последовательность ортогональных преобразований $Q$, и было бы гораздо легче, если бы существовало следующее разложение $A = QR$, тогда бы наша задача превратилась бы в:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}Ax - Q^{\\top}b||^2, \\, A = QR \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Q^{\\top}QRx - Q^{\\top}b||^2 \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Rx - Q^{\\top}b||^2 \\\\\n",
    "& \\text{argmin}_{x\\in\\mathbb{R}^2} \\||Rx - \\tilde b||^2, \\, Q^{\\top}b = \\tilde b \\\\\n",
    "& Rx = \\tilde b\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Оказывается такое разложение существует и называется $\\mathbf{QR}\\textbf{-разложение!}$, но о нем немного позже.\n",
    "\n",
    "#### Решение через нормальное уравнение\n",
    "\n",
    "Также можно попробовать решить данную задачу в текущем сетапе без подбора ортогонального преобразования из полученного ранее **нормального уравнения**.\n",
    "\n",
    "Для заданной матрицы $A \\in \\mathbb{R}^{m\\times n}$ с рангом $rank(A) = n$ и $b \\in \\mathbb{R}^{n}$, мы найдем такой вектор $x_{ls}$, удовлетворяющий нормальному уравнению. \n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& A^{\\top}Ax = A^{\\top}b \\\\\n",
    "& \\text{Посчитать} \\, C = A^{\\top}A \\\\\n",
    "& \\text{Посчитать} \\, d = A^{\\top}b \\\\\n",
    "& \\text{Разложение Холецкого:} \\, C = GG^{\\top} \\\\\n",
    "& \\text{Решить} \\, Gy =d, \\, G^{\\top}x_{ls} = y \n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13542faa",
   "metadata": {},
   "source": [
    "## QR разложение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe58e5",
   "metadata": {},
   "source": [
    "**Теорема. Любая $\\mathbf{A \\in \\mathbb{R}^{m \\times n}}$ может быть разложена на произведение ортогональной матрицы $\\mathbf{Q \\in \\mathbb{R}^{m \\times m}}$ и верхнетреугольной матрицы $\\mathbf{R \\in \\mathbb{R}^{m \\times n}}$.**\n",
    "\n",
    "Также существуют и другие записи в части размеров матриц разложения: $\\mathbf{Q \\in \\mathbb{R}^{m \\times n}}, \\mathbf{R \\in \\mathbb{R}^{n \\times n}}$ как вариант одной из записей, но данный вопрос касается программирования и численных алгоритов, в линейной алгебре матрица $Q$ - ортогональна (унитарна), а следовательно квадратна."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cff9d6",
   "metadata": {},
   "source": [
    "### QR разложение через процедуру Грамма-Шмидта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fd031",
   "metadata": {},
   "source": [
    "#### Внешнее произведение (outer product)\n",
    "\n",
    "Тут следует немного углубиться в детали внешнего произведения и его геометрических интепретаций.\n",
    "\n",
    "Начнем с более подобного рассмотрения этого разложения. $QR$ разложение можно представить как сумму внешних произведений (outer product) колонок матрицы $Q$ и строчек матрицы $R$. Само внешнее произведение $ww^{\\top}$ определяет проекцию по направлению вектора $w$. Проекция вектора на линию (или, в более общем смысле, подпространство) — это ближайшая точка на этой линии к исходному вектору. Если $w$ — вектор, определяющий направление линии, проекция любого вектора y на линию по w можно описать матрицей $ww^{\\top}$. Разумеется матрица $ww^{\\top}$ обладает всеми свойствами проекций. \n",
    "\n",
    "Если $w$ - единичный вектор, то матрица $P = ww^{\\top}$ задает матрицу проекции на линюю, задаваемую вектором $w$. $Px = w(w^{\\top}x)$, где $w^{\\top}x$ - скалярное произведение, которое задает скалярную проекцию векторф $x$ на вектор $w$, то есть длину вектора $x$, лежащую по направлению вектора $w$, далее умножая на $w$ получаем векторную проекцию вектора $x$ по направлению вектора $w$. Таким образом внешнее произведение $ww^{\\top}$ берет любой вектор $x$ и возращает ту часть, которая лежит по напралению $w$, эффективно исключая компоненты, ортогональные данному направлению, например мы хотем найти проекцию вектора $b = (5,3)$ на вектор $a = (1,2)$, посчитаем внешнее произведение:\n",
    "\n",
    "$\n",
    "aa^{\\top} = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 & 2\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{pmatrix}\n",
    "$\n",
    "и далее посчитаем матрично-векторное произведение \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "5 \\\\\n",
    "3\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "11 \\\\\n",
    "22\n",
    "\\end{pmatrix} = 11 \\begin{pmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "В данном примере, мы не задавали ограничение на единичность вектора, на который собирались делать отображение. И математически, мы посчитали следующее: $a^{\\top}b = ||a||||b||\\cos\\theta$ - угол между векторами не нормированный на длины векторов, тогда проекцией является длина катета треугольника на стороне вектора $a$ (расстояние от начала векторов до перпендикуляра) и это $proj_a(b) = ||b||\\cos\\theta = \\frac{a^{\\top}b}{||a||}$, а чтобы расчитать векторную составляющую компоненты $b$ по вектору $a$ надо домножить длину катета на нормированный вектор $\\frac{a}{||a||}$, и того $\\overset{\\rightharpoonup}{\\text{proj}}_a(b) = \\frac{aa^{\\top}}{a^{\\top}a}b$\n",
    "\n",
    "#### Продолжение процедуры Грамма Шмидта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7eb4aa",
   "metadata": {},
   "source": [
    "#### Преобразование Хаусхольдера \n",
    "\n",
    "$$\n",
    "w^{\\top}w = 1, \\quad P = \\mathbf{I} - 2ww^{\\top} - \\text{матрица Хаусхольдера}\n",
    "$$\n",
    "\n",
    "Для данной матрицы есть несколько полезных свойств, например **симметричность и ортогональность**:\n",
    "\n",
    "**Доказательство симметричности:**\n",
    "\n",
    "$\n",
    "P^{\\top} = (\\mathbf{I} - 2ww^{\\top})^{\\top} = \\mathbf{I} - 2(w^{\\top})^{\\top}w^{\\top} = \\mathbf{I} - 2ww^{\\top}\n",
    "$\n",
    "\n",
    "**Доказательство ортогональности:**\n",
    "\n",
    "$\n",
    "PP^{\\top} = (\\mathbf{I} - 2ww^{\\top})(\\mathbf{I} - 2ww^{\\top}) = \\mathbf{I} - 4ww^{\\top} + 4ww^{\\top}ww^{\\top} = \\mathbf{I} - 4ww^{\\top} + 4w(w^{\\top}w)w^{\\top} = \\mathbf{I} - 4ww^{\\top} + 4ww^{\\top} = \\mathbf{I}\n",
    "$\n",
    "\n",
    "И некоторые другие свойства касательно матрицы Хаусхольдера:\n",
    "\n",
    "a) $Pw = -w$ и для любого $x\\perp w, \\, w^{\\top}x = 0 \\rightarrow Px = x$ говорит нам о том, что матрица $P$ имеет собственное значение $\\lambda_1 = 1$ кратности $(n-1)$ и $\\lambda_{n} = -1$\n",
    "\n",
    "**Доказательство**.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "Pw = w - 2ww^{\\top}w = -w\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "б) $x,y: ||x|| = ||y||, \\, P = \\mathbf{I} - 2ww^{\\top} , \\, \\text{где} \\, w = \\frac{(x-y)}{||x-y||_2} $\n",
    "\n",
    "$Px = y$\n",
    "\n",
    "**Доказательство**. Предположим, что $Px = y$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x - &2\\frac{(x-y)}{||x-y||_2}\\frac{(x-y)^{\\top}}{||x-y||_2}x = y \\\\\n",
    "(x-y) - &\\frac{(x-y)(x-y)^{\\top}}{||x-y||^2_2}x = 0 \\\\\n",
    "\\frac{(x-y)}{||x-y||^2_2}&(||x-y||^2_2 - 2(x-y)^{\\top}x) = 0 \\\\\n",
    "\\frac{(x-y)}{||x-y||^2_2}&((x-y)(x-y)^{\\top} - 2(x-y)^{\\top}x) = 0 \\\\\n",
    "\\frac{(x-y)}{||x-y||^2_2}&(xx^{\\top}-xy^{\\top}-yx^{\\top}+yy^{\\top}-2x^{\\top}x+2y^{\\top}x) = 0 \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Что означает, что для любых векторов $x,y, ||x|| = ||y||$ существует матрица хаусхольдера с помощью которой мы можем $x \\rightarrow y$\n",
    "\n",
    "Матрица Хаусхолдера (или отражения Хаусхольдера) похожи на преобразования Гаусса в том, что они могут использоваться для обнуления выбранных компонентов вектора. В частности, предположим, что нам дан $x \\in \\mathbb{R}^{m}$ и мы хотим найти такую матрицу $P$:\n",
    "\n",
    "$\n",
    "Px = \n",
    "\\begin{pmatrix}\n",
    "||x||_2 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "Отнормируем матрицу Хаусхольдера и перепишем ее в общем виде без добавления ограничения на его норму $P = \\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w}$, вектор $w$ выбирается таким образом, что при применении преобразования Хаусхолдера к $x$ получается вектор, кратный стандартному базисному вектору $e_1 = (1, 0, \\dots, 0)^{\\top}$. Мы назначаем такой вектор $w = x + \\alpha e_1$ для некоторого скаляра $\\alpha$, где $e_1$ — первый стандартный базисный вектор в $\\mathbb{R}^m$. Выбор $\\alpha$ имеет решающее значение для обеспечения того, чтобы преобразованный вектор $Px$ имел нули в желаемых позициях. Для достижения нашей задачи мы накладываем такое условие, что $Px$ должно быть кратно $e_1$. Это приводит нас к решению для $\\alpha$, при котором коэффициент при $x$ в преобразованном векторе становится равным нулю.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "&w^{\\top}x = x^{\\top}x + \\alpha x_1 \\\\\n",
    "&w^{\\top}w = x^{\\top}x + 2\\alpha x_1 + \\alpha^2 \\\\\n",
    "&Px = x - 2\\frac{w^{\\top}x}{w^{\\top}w}w \\\\\n",
    "&Px = x - 2\\frac{(x + \\alpha e_1)(x^{\\top}x + \\alpha x_1 )}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2} \\\\\n",
    "&Px = (1 - \\frac{x^{\\top}x + \\alpha x_1}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2})x - 2\\alpha\\frac{w^{\\top}x}{w^{\\top}w}e_1 \\\\\n",
    "&Px = (\\frac{\\alpha^2 - ||x||_2^2}{x^{\\top}x + 2\\alpha x_1 + \\alpha^2})x - 2\\alpha\\frac{w^{\\top}x}{w^{\\top}w}e_1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Чтобы коэффициент при $x$ был нулевой выбираем $\\alpha = ||x||_2$, тогда:\n",
    "\n",
    "$\n",
    "w = x + ||x||_2 e_1 \\rightarrow Px = (\\mathbf{I} - 2\\frac{ww^{\\top}}{w^{\\top}w})x = ||x||_2e_1\n",
    "$\n",
    "\n",
    "\n",
    "**Пример**\n",
    "\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "4 & 4 & 2\\\\\n",
    "4 & 5 & 3\\\\\n",
    "2 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Для матрицы $A$ мы хотим превратить $x_1 = (4,4,2) \\rightarrow y_1 = (*,0,0)$, найдем этот вектор, $||x_1|| = 36 \\rightarrow y_1 = (6,0,0)$\n",
    "\n",
    "$$\n",
    "w = \\frac{\n",
    "\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}}{||\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}||} = \\begin{pmatrix} \\frac{-1}{6}\\\\ \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "w^{\\top}w = \\begin{bmatrix}\n",
    "\\frac{1}{6} & \\frac{-2}{6} & \\frac{-1}{6}\\\\\n",
    "\\frac{-2}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{-1}{6} & \\frac{2}{6} & \\frac{1}{6}\n",
    "\\end{bmatrix}\\\\\n",
    "\\newline\n",
    "P = \\mathbf{I} - 2w^{\\top}w  = \n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{4}{6} & - \\frac{2}{6} & -\\frac{4}{6}\\\\\n",
    "\\frac{2}{6} & -\\frac{4}{6} & \\frac{4}{6}\n",
    "\\end{bmatrix}\\\\\n",
    "P\\begin{pmatrix} 4\\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 6\\\\ 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Ну а для получения матрицы данного преобразования:\n",
    "\n",
    "$$\n",
    "PA = \\begin{bmatrix}\n",
    "6 & 7 & \\frac{13}{3}\\\\\n",
    "0 & -1 & -\\frac{5}{3}\\\\\n",
    "0 & 0 & \\frac{2}{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Аналогичные преобразования можно получать и для строчек (минусая столбцы) умножая на матрицу Хаусхольдера справа $AP$, аналогично сводя матрицу к треугольному виду\n",
    "\n",
    "Таким образом имеем $QR$ разложение для матрицы $A$:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "4 & 4 & 2\\\\\n",
    "4 & 5 & 3\\\\\n",
    "2 & 3 & 3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{6} & \\frac{4}{6} & \\frac{2}{6}\\\\\n",
    "\\frac{4}{6} & - \\frac{2}{6} & -\\frac{4}{6}\\\\\n",
    "\\frac{2}{6} & -\\frac{4}{6} & \\frac{4}{6}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "6 & 7 & \\frac{13}{3}\\\\\n",
    "0 & -1 & -\\frac{5}{3}\\\\\n",
    "0 & 0 & \\frac{2}{3}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd40ca",
   "metadata": {},
   "source": [
    "надо написать почему $w = x +ae$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da6311",
   "metadata": {},
   "source": [
    "#### Вращение Гивенса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2d9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
